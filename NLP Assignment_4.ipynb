{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?\n",
    "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length.\n",
    "\n",
    "Applications are speech recognition, machine translation, image captioning and question answering.\n",
    "\n",
    "In Sequence to vector Learning, RNN is trained to map an input sequence to an output vector which is not necessarily of the same length.\n",
    "\n",
    "In vector to Sequence Learning, RNN is trained to map an input vector to an output sequence which is not necessarily of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "The encoder-decoder architecture for recurrent neural networks is the standard neural machine translation method that rivals and in some cases outperforms classical statistical machine translation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How could you combine a convolutional neural network with an RNN to classify videos?\n",
    "Taking advantage of the strengths of both CNN and RNN, the combination outperforms those individual models. Another method to combine them together is to let RNN encode the input representation and feed the outputs into CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?\n",
    "\n",
    "# Static_rnn\n",
    "\n",
    "Internally, tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified.\n",
    "\n",
    "# Dynamic_rnn\n",
    "\n",
    "tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "Approach 1: create separate batches of 1 sequence, each batch with its own length. Feed each batch to the model individually.\n",
    "\n",
    "Approach 2: create a fixed length batch, fill the unused tail lenght of each sequence with 0, use the parameter mask_zero=True in the embedding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What is a common way to distribute training and execution of a deep RNN across multiple GPUs?\n",
    "The GPUs can all be on the same node or across multiple nodes. Only gradients are passed between the processes/GPUs. During training, each process loads its own mini-batch from disk and passes it to its GPU. Each GPU does its forward pass, then the gradients are all-reduced across the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
